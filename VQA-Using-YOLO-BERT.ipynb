{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8Dmy5Z/I2gZHRLaU0Cxut"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/"],"metadata":{"id":"MYP9_ztxTJyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install ultralytics transformers\n","import ultralytics\n","ultralytics.checks()"],"metadata":{"id":"WIqEAPOzTJvx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from transformers import BertTokenizer, BertModel, BertConfig\n","from ultralytics import YOLO\n","from collections import defaultdict\n","from PIL import Image\n","from tqdm import tqdm\n","import json\n","import csv\n","import os"],"metadata":{"id":"zOPDBPSMTJtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure CUDA (GPU support) is available if possible, else use CPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')"],"metadata":{"id":"Mm4BZiaITJow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the trained YOLOv8 model\n","yolo_model yolo_model = YOLO('/content/drive/MyDrive/00_PFE/Object_Detection/Training_Results/Yolov8-V6/Results/runs/train/experiment/weights/best.pt').to(device)"],"metadata":{"id":"LaOe618DTJmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the label mapping\n","label_mapping = [\n","    \"flooded\", \"non flooded\", \"flooded,non flooded\", \"Yes\", \"No\",\n","    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n","    \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\",\n","    \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\",\n","    \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\",\n","    \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\"\n","]\n","\n","question_type_mapping = {\n","    \"Condition_Recognition\": 0,\n","    \"Yes_No\": 1,\n","    \"Simple_Counting\": 2,\n","    \"Complex_Counting\": 3\n","}"],"metadata":{"id":"4Mi_xlYbTJkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to extract features from YOLOv8\n","def extract_yolo_features(image_path, model, device):\n","    results = model(image_path)\n","\n","    # Initialize lists to store extracted features\n","    boxes_list = []\n","    cls_list = []\n","\n","    for result in results:\n","        if result.boxes is not None:\n","            boxes = result.boxes.xyxy.to(device)  # Bounding box coordinates\n","            classes = result.boxes.cls.to(device)  # Class values\n","            boxes_list.append(boxes)\n","            cls_list.append(classes)\n","\n","    # Combine features into a single tensor\n","    if boxes_list:\n","        features = torch.cat([torch.cat(boxes_list), torch.cat(cls_list).unsqueeze(1)], dim=1)\n","    else:\n","        features = torch.empty((0, 5), device=device)\n","\n","    return features"],"metadata":{"id":"YFiVYa70TJg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VQADataset class\n","class VQADataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, tokenizer, transform=None):\n","        with open(annotations_file, 'r') as f:\n","            self.annotations = json.load(f)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.tokenizer = tokenizer\n","        self.img_to_annotations = self._group_by_image()\n","\n","    def _group_by_image(self):\n","        img_to_annotations = defaultdict(list)\n","        for idx, annotation in self.annotations.items():\n","            img_to_annotations[annotation['Image_ID']].append(annotation)\n","        return img_to_annotations\n","\n","    def __len__(self):\n","        return len(self.img_to_annotations)\n","\n","    def __getitem__(self, idx):\n","        image_id = list(self.img_to_annotations.keys())[idx]\n","        annotations = self.img_to_annotations[image_id]\n","        img_path = os.path.join(self.img_dir, image_id)\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        questions = []\n","        answers = []\n","        question_types = []\n","        for annotation in annotations:\n","            inputs = self.tokenizer.encode_plus(\n","                annotation['Question'],\n","                add_special_tokens=True,\n","                return_tensors='pt',\n","                padding='max_length',\n","                truncation=True,\n","                max_length=64\n","            )\n","            question = inputs['input_ids'].squeeze(0).to(device)\n","            attention_mask = inputs['attention_mask'].squeeze(0).to(device)\n","            answer_text = str(annotation['Ground_Truth'])\n","            answer_idx = label_mapping.index(answer_text)\n","            question_type_idx = question_type_mapping[annotation['Question_Type']]\n","            questions.append((question, attention_mask))\n","            answers.append(torch.tensor(answer_idx, device=device))\n","            question_types.append(torch.tensor(question_type_idx, device=device))\n","        return {\n","            'image_path': img_path,\n","            'questions': questions,\n","            'attention_masks': [am for _, am in questions],\n","            'answers': torch.stack(answers),\n","            'question_types': torch.stack(question_types)\n","        }"],"metadata":{"id":"FJZ_TbZyTJef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_collate_fn(batch):\n","    batch_image_paths = [item['image_path'] for item in batch]\n","    batch_questions = [q for item in batch for q, _ in item['questions']]\n","    batch_attention_masks = [am for item in batch for _, am in item['questions']]\n","    batch_answers = torch.cat([item['answers'] for item in batch])\n","    batch_question_types = torch.cat([item['question_types'] for item in batch])\n","    num_questions_per_image = [len(item['questions']) for item in batch]\n","    return {\n","        'image_paths': batch_image_paths,\n","        'questions': batch_questions,\n","        'attention_masks': batch_attention_masks,\n","        'answers': batch_answers,\n","        'question_types': batch_question_types,\n","        'num_questions_per_image': num_questions_per_image\n","    }"],"metadata":{"id":"Wg9-IzIKTJax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VQAModel(nn.Module):\n","    def __init__(self, bert_model, yolo_input_dim, hidden_dim, combined_dim, vocab_size, num_question_types):\n","        super(VQAModel, self).__init__()\n","        self.bert_model = bert_model\n","        self.fc_yolo = nn.Linear(yolo_input_dim, hidden_dim)\n","        self.fc_question_type = nn.Embedding(num_question_types, hidden_dim)\n","        self.fc_proj = nn.Linear(hidden_dim * 2 + 768, combined_dim)  # Project to BERT input dimension\n","\n","    def forward(self, image_features, questions, attention_masks, question_types, num_questions_per_image):\n","        image_features = [self.fc_yolo(image_feature) for image_feature in image_features]\n","        image_features = torch.stack(image_features)\n","\n","        text_features = [self.bert_model(question.unsqueeze(0).to(image_features.device), attention_mask=attention_mask.unsqueeze(0).to(image_features.device)).pooler_output for question, attention_mask in zip(questions, attention_masks)]\n","        text_features = torch.cat(text_features, dim=0)\n","\n","        expanded_image_features = []\n","        for image_feature, num_questions in zip(image_features, num_questions_per_image):\n","            expanded_image_features.append(image_feature.repeat(num_questions, 1))\n","        expanded_image_features = torch.cat(expanded_image_features, dim=0)\n","\n","        question_type_features = self.fc_question_type(question_types)\n","\n","        combined_features = torch.cat((expanded_image_features, text_features, question_type_features), dim=1)\n","        projected_features = self.fc_proj(combined_features)\n","\n","        # Reshape projected features to match BERT's expected input dimensions\n","        batch_size = projected_features.size(0)\n","        seq_length = 1  # Since we're treating each feature set as a single \"sequence\"\n","        projected_features = projected_features.view(batch_size, seq_length, -1)\n","\n","        # Prepare BERT inputs\n","        extended_attention_mask = torch.ones((batch_size, seq_length), device=projected_features.device)\n","\n","        # Pass through BERT model\n","        outputs = self.bert_model(inputs_embeds=projected_features, attention_mask=extended_attention_mask)\n","        pooled_output = outputs.pooler_output\n","\n","        return pooled_output"],"metadata":{"id":"9eKBlUM5TJW3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize tokenizer, BERT model, and VQA model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n","num_classes = len(label_mapping)\n","num_question_types = len(question_type_mapping)\n","hidden_dim = 256\n","combined_dim = 768  # Adjust based on BERT input dimensions\n","vqa_model = VQAModel(bert_model=bert_model, yolo_input_dim=5, hidden_dim=hidden_dim, combined_dim=combined_dim, vocab_size=num_classes, num_question_types=num_question_types).to(device)"],"metadata":{"id":"gK6xLQFUTxLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define optimizer and loss function\n","optimizer = torch.optim.Adam(vqa_model.parameters(), lr=0.0001)\n","criterion = torch.nn.CrossEntropyLoss()"],"metadata":{"id":"UG_1YSIFT76S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize dataset and dataloader\n","annotations_file = '/content/drive/MyDrive/00_PFE/DataSet/Visual_Question_Answering /FloodNet Challenge @ EARTHVISION 2021 - Track 2/Questions/Training Question.json'\n","img_dir = '/content/drive/MyDrive/00_PFE/DataSet/Visual_Question_Answering /FloodNet Challenge @ EARTHVISION 2021 - Track 2/Images/Train_Image'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","dataset = VQADataset(annotations_file, img_dir, bert_tokenizer, transform)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)"],"metadata":{"id":"SA6JXg2KT-3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup logging\n","def setup_logging():\n","    with open('log.csv', 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([\"Epoch\", \"Average Loss\", \"Average Accuracy\"])\n","\n","def log_epoch(epoch, avg_loss, avg_accuracy):\n","    with open('log.csv', 'a', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([epoch, f\"{avg_loss:.4f}\", f\"{avg_accuracy * 100:.2f}%\"])"],"metadata":{"id":"CAi-QafmT-0Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to compute accuracy\n","def compute_accuracy(predictions, labels):\n","    _, predicted = torch.max(predictions, 1)\n","    correct = (predicted == labels).float().sum()\n","    return correct / labels.size(0)"],"metadata":{"id":"Vx5xPKZMUBhG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHat9nB2Sb_d"},"outputs":[],"source":["setup_logging()\n","num_epochs = 10\n","best_accuracy = 0.0\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    total_accuracy = 0\n","    total_batches = 0\n","    vqa_model.train()\n","    with tqdm(dataloader, desc=f\"Epoch {epoch + 1}\") as pbar:\n","        for batch in pbar:\n","            image_paths = batch['image_paths']\n","            questions = batch['questions']\n","            attention_masks = batch['attention_masks']\n","            answers = batch['answers']\n","            question_types = batch['question_types']\n","            num_questions_per_image = batch['num_questions_per_image']\n","\n","            # Extract features using YOLOv8\n","            image_features_list = []\n","            for image_path in image_paths:\n","                features = extract_yolo_features(image_path, yolo_model, device)\n","                if features.nelement() == 0:\n","                    features = torch.zeros((1, 5), device=device)  # Handle no detections case\n","                image_features_list.append(features.mean(dim=0))\n","            image_features = torch.stack(image_features_list)\n","\n","            optimizer.zero_grad()\n","            outputs = vqa_model(image_features, questions, attention_masks, question_types, num_questions_per_image)\n","            loss = criterion(outputs, answers)\n","            if torch.isnan(loss):\n","                print(f\"Encountered NaN loss, skipping this batch\")\n","                continue\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_loss = loss.item()\n","            batch_accuracy = compute_accuracy(outputs, answers).item()\n","            total_loss += batch_loss\n","            total_accuracy += batch_accuracy\n","            total_batches += 1\n","            pbar.set_postfix(Loss=batch_loss, Accuracy=f\"{batch_accuracy:.4f}\")\n","\n","    avg_loss = total_loss / total_batches\n","    avg_accuracy = total_accuracy / total_batches\n","    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}\")\n","\n","    log_epoch(epoch + 1, avg_loss, avg_accuracy)\n","\n","    # Save the best model\n","    if avg_accuracy > best_accuracy:\n","        best_accuracy = avg_accuracy\n","        torch.save(vqa_model.state_dict(), f\"/content/drive/MyDrive/00_PFE/VQA/Code-V3/VQAModel_Best.pth\")\n","\n","print(\"Training complete!\")"]}]}